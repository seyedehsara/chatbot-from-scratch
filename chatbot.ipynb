{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1245709,"sourceType":"datasetVersion","datasetId":715041}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    datasets==2.11.0 \\\n    evaluate==0.4.0 \\\n    rouge_score==0.1.2 \\\n    loralib==0.1.1 \\\n    peft==0.3.0 --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:38:56.511802Z","iopub.execute_input":"2025-03-05T16:38:56.512251Z","iopub.status.idle":"2025-03-05T16:41:34.707003Z","shell.execute_reply.started":"2025-03-05T16:38:56.512219Z","shell.execute_reply":"2025-03-05T16:41:34.705193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nimport string\nfrom nltk.corpus import stopwords\nimport pandas as pd \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:58:21.531902Z","iopub.execute_input":"2025-03-05T16:58:21.532299Z","iopub.status.idle":"2025-03-05T16:58:21.537985Z","shell.execute_reply.started":"2025-03-05T16:58:21.532269Z","shell.execute_reply":"2025-03-05T16:58:21.536649Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"importing dataset","metadata":{}},{"cell_type":"code","source":"columns = ['question', 'answer']\ndf = pd.read_csv('/kaggle/input/simple-dialogs-for-chatbot/dialogs.txt', sep='\\t', names=columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:03:11.353917Z","iopub.execute_input":"2025-03-05T17:03:11.354289Z","iopub.status.idle":"2025-03-05T17:03:11.368134Z","shell.execute_reply.started":"2025-03-05T17:03:11.354259Z","shell.execute_reply":"2025-03-05T17:03:11.366993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndf.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:03:42.903953Z","iopub.execute_input":"2025-03-05T17:03:42.904300Z","iopub.status.idle":"2025-03-05T17:03:42.914649Z","shell.execute_reply.started":"2025-03-05T17:03:42.904275Z","shell.execute_reply":"2025-03-05T17:03:42.913227Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"EDA\nchecking for missing value\ncheck datatype,get overview","metadata":{}},{"cell_type":"code","source":"print(df.dtypes)\nprint(df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:08:04.111015Z","iopub.execute_input":"2025-03-05T17:08:04.111398Z","iopub.status.idle":"2025-03-05T17:08:04.120440Z","shell.execute_reply.started":"2025-03-05T17:08:04.111370Z","shell.execute_reply":"2025-03-05T17:08:04.119144Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"convert object to string","metadata":{}},{"cell_type":"code","source":"df['question'] = df['question'].astype(\"string\")\ndf['answer'] = df['answer'].astype(\"string\")\nprint(df.dtypes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:34:17.831497Z","iopub.execute_input":"2025-03-05T17:34:17.831830Z","iopub.status.idle":"2025-03-05T17:34:17.839975Z","shell.execute_reply.started":"2025-03-05T17:34:17.831807Z","shell.execute_reply":"2025-03-05T17:34:17.838716Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"word frequency analysis","metadata":{}},{"cell_type":"code","source":"import re\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport nltk\n\n# Download stopwords if not already downloaded\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:36:58.595949Z","iopub.execute_input":"2025-03-05T17:36:58.596314Z","iopub.status.idle":"2025-03-05T17:36:58.746439Z","shell.execute_reply.started":"2025-03-05T17:36:58.596289Z","shell.execute_reply":"2025-03-05T17:36:58.745423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_text(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n    words = text.split()  # Tokenize (split into words)\n    words = [word for word in words if word not in stop_words]  # Remove stopwords\n    return words\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:37:47.095909Z","iopub.execute_input":"2025-03-05T17:37:47.097181Z","iopub.status.idle":"2025-03-05T17:37:47.102356Z","shell.execute_reply.started":"2025-03-05T17:37:47.097141Z","shell.execute_reply":"2025-03-05T17:37:47.101244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_questions = df['question'].dropna().apply(preprocess_text).sum()\nall_answers = df['answer'].dropna().apply(preprocess_text).sum()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:38:17.861777Z","iopub.execute_input":"2025-03-05T17:38:17.862150Z","iopub.status.idle":"2025-03-05T17:38:18.048845Z","shell.execute_reply.started":"2025-03-05T17:38:17.862124Z","shell.execute_reply":"2025-03-05T17:38:18.047809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"question_freq = Counter(all_questions)\nanswer_freq = Counter(all_answers)\n\n# Get the 10 most common words\nprint(\"Most Common Words in Questions:\", question_freq.most_common(10))\nprint(\"Most Common Words in Answers:\", answer_freq.most_common(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:38:52.407943Z","iopub.execute_input":"2025-03-05T17:38:52.408312Z","iopub.status.idle":"2025-03-05T17:38:52.419018Z","shell.execute_reply.started":"2025-03-05T17:38:52.408288Z","shell.execute_reply":"2025-03-05T17:38:52.417849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_wordcloud(word_freq, title):\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()\n\nplot_wordcloud(question_freq, \"Most Common Words in Questions\")\nplot_wordcloud(answer_freq, \"Most Common Words in Answers\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:39:27.506779Z","iopub.execute_input":"2025-03-05T17:39:27.507192Z","iopub.status.idle":"2025-03-05T17:39:30.048074Z","shell.execute_reply.started":"2025-03-05T17:39:27.507159Z","shell.execute_reply":"2025-03-05T17:39:30.046802Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"using spacy for lemmatization","metadata":{}},{"cell_type":"code","source":"!pip install spacy\nimport spacy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:51:39.102517Z","iopub.execute_input":"2025-03-05T17:51:39.102944Z","iopub.status.idle":"2025-03-05T17:51:46.944421Z","shell.execute_reply.started":"2025-03-05T17:51:39.102916Z","shell.execute_reply":"2025-03-05T17:51:46.943164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python -m spacy download en_core_web_sm\nnlp = spacy.load(\"en_core_web_sm\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:51:55.741478Z","iopub.execute_input":"2025-03-05T17:51:55.741972Z","iopub.status.idle":"2025-03-05T17:52:07.650928Z","shell.execute_reply.started":"2025-03-05T17:51:55.741943Z","shell.execute_reply":"2025-03-05T17:52:07.649657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()  # Lowercase\n    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n    text = re.sub(r'\\d+', '', text)  # Remove numbers\n    doc = nlp(text)  # Process text with spaCy\n    words = [token.lemma_ for token in doc if token.text not in stop_words]\n    return \" \".join(words)\n\ndf['clean_question'] = df['question'].dropna().apply(clean_text)\ndf['clean_answer'] = df['answer'].dropna().apply(clean_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:52:43.165428Z","iopub.execute_input":"2025-03-05T17:52:43.165908Z","iopub.status.idle":"2025-03-05T17:53:36.562750Z","shell.execute_reply.started":"2025-03-05T17:52:43.165877Z","shell.execute_reply":"2025-03-05T17:53:36.561622Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"convert string to embeddings","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\nquestion_embeddings = model.encode(df['clean_question'].tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:56:55.285027Z","iopub.execute_input":"2025-03-05T17:56:55.285598Z","iopub.status.idle":"2025-03-05T17:57:07.101961Z","shell.execute_reply.started":"2025-03-05T17:56:55.285559Z","shell.execute_reply":"2025-03-05T17:57:07.100837Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"retrieving the top-matching answer using FAISS and generating a response using an LLM.","metadata":{}},{"cell_type":"code","source":"!pip install faiss-cpu sentence-transformers transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:00:54.562062Z","iopub.execute_input":"2025-03-05T18:00:54.562500Z","iopub.status.idle":"2025-03-05T18:01:01.022897Z","shell.execute_reply.started":"2025-03-05T18:00:54.562467Z","shell.execute_reply":"2025-03-05T18:01:01.021440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import faiss\nimport numpy as np\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import pipeline\n\n# Load sentence transformer model for embeddings\nembedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:01:21.807256Z","iopub.execute_input":"2025-03-05T18:01:21.807674Z","iopub.status.idle":"2025-03-05T18:01:23.220016Z","shell.execute_reply.started":"2025-03-05T18:01:21.807636Z","shell.execute_reply":"2025-03-05T18:01:23.218626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Encode all questions into embeddings\nquestion_embeddings = embedder.encode(df['question'].tolist())\n\n# Convert embeddings to FAISS index\ndimension = question_embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)  # L2 distance index\nindex.add(np.array(question_embeddings))  # Add embeddings to index\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:02:16.240149Z","iopub.execute_input":"2025-03-05T18:02:16.240569Z","iopub.status.idle":"2025-03-05T18:02:28.955495Z","shell.execute_reply.started":"2025-03-05T18:02:16.240540Z","shell.execute_reply":"2025-03-05T18:02:28.953508Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline\n\n# Load the LLaMA 2 model for text generation\nllm = pipeline(\n    \"text-generation\",\n    model=\"tiiuae/falcon-7b-instruct\",\n    device=0 if torch.cuda.is_available() else -1\n)\ndef get_similar_answer(user_question, k=3):\n    # Convert user question to an embedding\n    user_embedding = embedder.encode([user_question])\n\n    # Search FAISS index for top K similar questions\n    _, idx = index.search(np.array(user_embedding), k)\n    \n    # Retrieve top-matching answers\n    top_answers = [df['answer'].iloc[i] for i in idx[0]]\n\n    # Combine retrieved answers for LLM input\n    prompt = f\"User Question: {user_question}\\nRetrieved Answers: {' '.join(top_answers)}\\nProvide a helpful response based on the retrieved answers.\"\n\n    # Generate a response using LLM\n    response = llm(prompt, max_length=100, num_return_sequences=1)[0][\"generated_text\"]\n    \n    return response\n\n# Test the chatbot\nuser_input = \"What is machine learning?\"\nprint(get_similar_answer(user_input))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:09:11.914537Z","iopub.execute_input":"2025-03-05T18:09:11.915078Z","iopub.status.idle":"2025-03-05T18:13:09.528582Z","shell.execute_reply.started":"2025-03-05T18:09:11.915042Z","shell.execute_reply":"2025-03-05T18:13:09.527279Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"finetuning with LoRA","metadata":{}},{"cell_type":"code","source":"pip install peft transformers datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:19:03.211440Z","iopub.execute_input":"2025-03-05T18:19:03.213188Z","iopub.status.idle":"2025-03-05T18:19:11.841731Z","shell.execute_reply.started":"2025-03-05T18:19:03.213055Z","shell.execute_reply":"2025-03-05T18:19:11.840096Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nfrom transformers import Trainer, TrainingArguments\n\n# Load the tokenizer and model for Falcon 7B\nmodel_name = \"tiiuae/falcon-7b-instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# LoRA Configuration\nconfig = LoraConfig(\n    r=16,  # rank (hyperparameter, adjust based on your need)\n    lora_alpha=32,  # scaling factor\n    target_modules=[\"q_proj\", \"v_proj\"],  # specific layers to apply LoRA\n    lora_dropout=0.1,  # dropout rate\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, config)\n\n# Load your custom Q&A data \ndata = {\n    'question': df['question'].tolist(),\n    'answer': df['answer'].tolist()\n}\n\n# Convert to Hugging Face dataset\ndataset = Dataset.from_dict(data)\n\n# Tokenize the dataset\ndef tokenize_data(example):\n    prompt = f\"Question: {example['question']}\\nAnswer: {example['answer']}\"\n    return tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n\ntokenized_dataset = dataset.map(tokenize_data, batched=True)\n\n# Prepare Training Arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    eval_dataset=tokenized_dataset,  # You can provide a separate validation dataset\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Save the fine-tuned model\ntrainer.save_model(\"./fine_tuned_model\")\n\n# Load the fine-tuned model for inference\nmodel = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_model\")\n\n# LLM pipeline for text generation after fine-tuning\nllm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n\ndef get_similar_answer(user_question, k=3):\n    # Convert user question to an embedding\n    user_embedding = embedder.encode([user_question])\n\n    # Search FAISS index for top K similar questions\n    _, idx = index.search(np.array(user_embedding), k)\n    \n    # Retrieve top-matching answers\n    top_answers = [df['answer'].iloc[i] for i in idx[0]]\n\n    # Combine retrieved answers for LLM input\n    prompt = f\"User Question: {user_question}\\nRetrieved Answers: {' '.join(top_answers)}\\nProvide a helpful response based on the retrieved answers.\"\n\n    # Generate a response using fine-tuned LLM\n    response = llm(prompt, max_length=100, num_return_sequences=1)[0][\"generated_text\"]\n    \n    return response\n\n# Test the fine-tuned chatbot\nuser_input = \"What is machine learning?\"\nprint(get_similar_answer(user_input))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T18:21:35.990276Z","iopub.execute_input":"2025-03-05T18:21:35.990617Z","iopub.status.idle":"2025-03-05T18:21:36.012197Z","shell.execute_reply.started":"2025-03-05T18:21:35.990590Z","shell.execute_reply":"2025-03-05T18:21:36.010280Z"}},"outputs":[],"execution_count":null}]}